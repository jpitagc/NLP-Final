{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EdcCTalUVYvz"
      },
      "source": [
        "# NLP "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SVjsN-GirH3L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este notebook contiene de nuevo el pipeline de preprocesado con el fin de extraer los valores para el dato title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "VnakVmuZrMkN",
        "outputId": "d4c56499-2199-4b79-e47e-19ee6c6b5149"
      },
      "outputs": [],
      "source": [
        "corpus_df = pd.read_excel('./projects.xlsx')\n",
        "corpus_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GXimGUWErQBV",
        "outputId": "420bdfee-b235-4773-bbf3-31012450c5a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_path</th>\n",
              "      <th>title</th>\n",
              "      <th>code</th>\n",
              "      <th>full_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>livestock cloning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/27/79/483/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>plant cloning</td>\n",
              "      <td>1275.0</td>\n",
              "      <td>/27/79/483/1275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>agricultural genetics</td>\n",
              "      <td>483.0</td>\n",
              "      <td>/27/79/483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>biomass</td>\n",
              "      <td>481.0</td>\n",
              "      <td>/27/79/481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>marker assisted selection</td>\n",
              "      <td>487.0</td>\n",
              "      <td>/27/79/487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           full_path  \\\n",
              "0  /agricultural sciences/agricultural biotechnol...   \n",
              "1  /agricultural sciences/agricultural biotechnol...   \n",
              "2  /agricultural sciences/agricultural biotechnol...   \n",
              "3  /agricultural sciences/agricultural biotechnol...   \n",
              "4  /agricultural sciences/agricultural biotechnol...   \n",
              "\n",
              "                       title    code        full_code  \n",
              "0          livestock cloning     NaN      /27/79/483/  \n",
              "1              plant cloning  1275.0  /27/79/483/1275  \n",
              "2      agricultural genetics   483.0       /27/79/483  \n",
              "3                    biomass   481.0       /27/79/481  \n",
              "4  marker assisted selection   487.0       /27/79/487  "
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_df_labels = pd.read_excel('./SciVocCodes.xlsx')\n",
        "corpus_df_labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FuiLuV0psdty",
        "outputId": "52aa11eb-17ff-4c5e-c4a8-4f383b6e284f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_path</th>\n",
              "      <th>title</th>\n",
              "      <th>code</th>\n",
              "      <th>full_code</th>\n",
              "      <th>first_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>livestock cloning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/27/79/483/</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>plant cloning</td>\n",
              "      <td>1275.0</td>\n",
              "      <td>/27/79/483/1275</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>agricultural genetics</td>\n",
              "      <td>483.0</td>\n",
              "      <td>/27/79/483</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>biomass</td>\n",
              "      <td>481.0</td>\n",
              "      <td>/27/79/481</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/agricultural sciences/agricultural biotechnol...</td>\n",
              "      <td>marker assisted selection</td>\n",
              "      <td>487.0</td>\n",
              "      <td>/27/79/487</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           full_path  \\\n",
              "0  /agricultural sciences/agricultural biotechnol...   \n",
              "1  /agricultural sciences/agricultural biotechnol...   \n",
              "2  /agricultural sciences/agricultural biotechnol...   \n",
              "3  /agricultural sciences/agricultural biotechnol...   \n",
              "4  /agricultural sciences/agricultural biotechnol...   \n",
              "\n",
              "                       title    code        full_code first_code  \n",
              "0          livestock cloning     NaN      /27/79/483/         27  \n",
              "1              plant cloning  1275.0  /27/79/483/1275         27  \n",
              "2      agricultural genetics   483.0       /27/79/483         27  \n",
              "3                    biomass   481.0       /27/79/481         27  \n",
              "4  marker assisted selection   487.0       /27/79/487         27  "
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_df_labels['first_code'] = corpus_df_labels.apply(lambda row: re.findall(r'\\d+', str(row.full_code))[0], axis = 1)\n",
        "corpus_df_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ja5W0rgrTUE"
      },
      "source": [
        "Drop NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "8zNobsP5rSyL"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df = corpus_df.dropna(subset=['euroSciVocCode'])\n",
        "corpus_df = corpus_df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KupIQgbFrrWM"
      },
      "source": [
        "Add columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "72IQSrFYyBRA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def main_theme(row):\n",
        "  results = re.findall(r'\\d+', row.euroSciVocCode)\n",
        "  return int(corpus_df_labels.loc[corpus_df_labels['code'] == int(results[0])].first_code.values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "CrcXtcv9vLnj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def rest_of_themes(row):\n",
        "  results = re.findall(r'\\d+', row.euroSciVocCode)\n",
        "  x = []\n",
        "  for i in range(1,len(results)):\n",
        "    new_theme = int(corpus_df_labels.loc[corpus_df_labels['code'] == int(results[i])].first_code.values[0])\n",
        "    if new_theme != row.main_theme and new_theme not in x:\n",
        "      x.append(new_theme)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "o76O9loZvqI1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df['main_theme'] = corpus_df.apply(main_theme, axis=1)\n",
        "corpus_df['rest_themes'] = corpus_df.apply(rest_of_themes, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr49hO65-d8c"
      },
      "source": [
        "# Pipeline de preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08aT6Hj1OK96"
      },
      "source": [
        "Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH7FqhWpJ4TR",
        "outputId": "6b675f3e-00b3-400c-e119-e182bf6aca9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/pita/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/pita/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/pita/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "packages = ['punkt', 'stopwords', 'omw-1.4', 'wordnet']\n",
        "for package in packages:\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/' + package)\n",
        "    except LookupError:\n",
        "        nltk.download(package)\n",
        "try:\n",
        "  import lxml\n",
        "except ModuleNotFoundError:\n",
        "  %pip install lxml\n",
        "try:\n",
        "  import contractions\n",
        "except ModuleNotFoundError:\n",
        "  %pip install contractions\n",
        "  import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {
        "id": "qBzfDUSQLSfj"
      },
      "outputs": [],
      "source": [
        "# To wrap long text lines\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "  \n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pugaQW8OUdl"
      },
      "source": [
        "Cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bek0Ww6tG6m3",
        "outputId": "4010a57d-bcd7-4baf-87e8-c3384787fa48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def wrangle_text(text):\n",
        "\n",
        "    '''\n",
        "    Eliminates contractions and html markers.\n",
        "    '''\n",
        "\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r'https://\\S+|www\\.\\S+', '', text)\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_text(text):\n",
        "\n",
        "    '''\n",
        "    Preprocessing pipeline.\n",
        "    '''\n",
        "\n",
        "    text = wrangle_text(text)\n",
        "    tokens = wordpunct_tokenize(text)\n",
        "    tokens_filtered = [word.lower() for word in tokens if word.isalnum()]\n",
        "    lemmatized_text = [wnl.lemmatize(word) for word in tokens_filtered]\n",
        "    clean = [word for word in lemmatized_text if word not in stopwords_en]\n",
        "    \n",
        "    return clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 572,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df['title&summary'] = corpus_df['title'] + '. ' + corpus_df['summary']\n",
        "corpus_df['clean_summary'] = corpus_df['summary'].apply(prepare_text)\n",
        "corpus_df['clean_title'] = corpus_df['title'].apply(prepare_text)\n",
        "corpus_df['clean_title&summary'] = corpus_df['title&summary'].apply(prepare_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xfymJ1Og17"
      },
      "source": [
        "N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim==4.2.0\n",
            "  Downloading gensim-4.2.0-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0 MB 3.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting smart-open>=1.8.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim==4.2.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from gensim==4.2.0) (1.18.5)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.2.0 smart-open-6.3.0\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim==4.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 566,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PBJvSJf3Lgmq",
        "outputId": "8e3b38ef-a2ee-413b-db47-1617145f7ed1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "def ngrams(corpus, phrase_model): \n",
        "    corpus = [el for el in phrase_model[corpus]]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 576,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "phrase_model = Phrases(corpus_df['clean_title&summary'], min_count=2)\n",
        "corpus_df['clean_summary'] = ngrams(corpus_df.clean_summary, phrase_model)\n",
        "corpus_df['clean_title'] =  ngrams(corpus_df.clean_title,phrase_model)\n",
        "corpus_df['clean_title&summary'] =  ngrams(corpus_df['clean_title&summary'],phrase_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df.to_csv('./cleaned_data.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lEjbarvf-rET"
      },
      "source": [
        "# Representación vectorial Word Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxgpzAjlAtIh"
      },
      "source": [
        "## FastText Pretrained Full FB model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>projectID</th>\n",
              "      <th>acronym</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>startDate</th>\n",
              "      <th>endDate</th>\n",
              "      <th>totalCost</th>\n",
              "      <th>ecMaxContribution</th>\n",
              "      <th>masterCall</th>\n",
              "      <th>subCall</th>\n",
              "      <th>...</th>\n",
              "      <th>coordinatorCountry</th>\n",
              "      <th>euroSciVocCode</th>\n",
              "      <th>publicationID</th>\n",
              "      <th>patentID</th>\n",
              "      <th>main_theme</th>\n",
              "      <th>rest_themes</th>\n",
              "      <th>clean_summary</th>\n",
              "      <th>clean_title</th>\n",
              "      <th>title&amp;summary</th>\n",
              "      <th>clean_title&amp;summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>115843</td>\n",
              "      <td>EbolaMoDRAD</td>\n",
              "      <td>Ebola Virus: Modern Approaches for developing ...</td>\n",
              "      <td>The current Ebola Virus Disease (EVD) outbreak...</td>\n",
              "      <td>2015-02-01</td>\n",
              "      <td>2018-01-31</td>\n",
              "      <td>4300935.0</td>\n",
              "      <td>4300935.0</td>\n",
              "      <td>H2020-JTI-IMI2-2014-02-single-stage</td>\n",
              "      <td>H2020-JTI-IMI2-2014-02-single-stage</td>\n",
              "      <td>...</td>\n",
              "      <td>SE</td>\n",
              "      <td>[155, 56306972, 325, 137, 1609]</td>\n",
              "      <td>['115843_202840_PUBLI', '115843_202838_PUBLI',...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>[23]</td>\n",
              "      <td>['current', 'ebola_virus', 'disease_evd', 'out...</td>\n",
              "      <td>['ebola_virus', 'modern', 'approach', 'develop...</td>\n",
              "      <td>Ebola Virus: Modern Approaches for developing ...</td>\n",
              "      <td>['ebola_virus', 'modern', 'approach', 'develop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>115910</td>\n",
              "      <td>PERISCOPE</td>\n",
              "      <td>PERtussIS COrrelates of Protection Europe - So...</td>\n",
              "      <td>Pertussis vaccines have been very successful i...</td>\n",
              "      <td>2016-03-01</td>\n",
              "      <td>2022-08-31</td>\n",
              "      <td>29926687.0</td>\n",
              "      <td>21000000.0</td>\n",
              "      <td>H2020-JTI-IMI2-2015-03-two-stage</td>\n",
              "      <td>H2020-JTI-IMI2-2015-03-two-stage</td>\n",
              "      <td>...</td>\n",
              "      <td>NL</td>\n",
              "      <td>[137, 1439, 44109686, 48479582]</td>\n",
              "      <td>['115910_1008742_PUBLI', '115910_629396_PUBLI'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>[]</td>\n",
              "      <td>['pertussis_vaccine', 'successful', 'reducing'...</td>\n",
              "      <td>['pertussis', 'correlate_protection', 'europe'...</td>\n",
              "      <td>PERtussIS COrrelates of Protection Europe - So...</td>\n",
              "      <td>['pertussis', 'correlate_protection', 'europe'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   projectID      acronym                                              title  \\\n",
              "0     115843  EbolaMoDRAD  Ebola Virus: Modern Approaches for developing ...   \n",
              "1     115910    PERISCOPE  PERtussIS COrrelates of Protection Europe - So...   \n",
              "\n",
              "                                             summary   startDate     endDate  \\\n",
              "0  The current Ebola Virus Disease (EVD) outbreak...  2015-02-01  2018-01-31   \n",
              "1  Pertussis vaccines have been very successful i...  2016-03-01  2022-08-31   \n",
              "\n",
              "    totalCost  ecMaxContribution                           masterCall  \\\n",
              "0   4300935.0          4300935.0  H2020-JTI-IMI2-2014-02-single-stage   \n",
              "1  29926687.0         21000000.0     H2020-JTI-IMI2-2015-03-two-stage   \n",
              "\n",
              "                               subCall  ... coordinatorCountry  \\\n",
              "0  H2020-JTI-IMI2-2014-02-single-stage  ...                 SE   \n",
              "1     H2020-JTI-IMI2-2015-03-two-stage  ...                 NL   \n",
              "\n",
              "                    euroSciVocCode  \\\n",
              "0  [155, 56306972, 325, 137, 1609]   \n",
              "1  [137, 1439, 44109686, 48479582]   \n",
              "\n",
              "                                       publicationID patentID main_theme  \\\n",
              "0  ['115843_202840_PUBLI', '115843_202838_PUBLI',...      NaN         21   \n",
              "1  ['115910_1008742_PUBLI', '115910_629396_PUBLI'...      NaN         21   \n",
              "\n",
              "  rest_themes                                      clean_summary  \\\n",
              "0        [23]  ['current', 'ebola_virus', 'disease_evd', 'out...   \n",
              "1          []  ['pertussis_vaccine', 'successful', 'reducing'...   \n",
              "\n",
              "                                         clean_title  \\\n",
              "0  ['ebola_virus', 'modern', 'approach', 'develop...   \n",
              "1  ['pertussis', 'correlate_protection', 'europe'...   \n",
              "\n",
              "                                       title&summary  \\\n",
              "0  Ebola Virus: Modern Approaches for developing ...   \n",
              "1  PERtussIS COrrelates of Protection Europe - So...   \n",
              "\n",
              "                                 clean_title&summary  \n",
              "0  ['ebola_virus', 'modern', 'approach', 'develop...  \n",
              "1  ['pertussis', 'correlate_protection', 'europe'...  \n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_df = pd.read_csv('./cleaned_data.csv')\n",
        "corpus_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install fasttext\n",
        "#fasttext.util.download_model('en', if_exists='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "THTg7wl81h4y",
        "outputId": "d367c3d7-7a7e-44be-e371-b4ef5c0003e8"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 588,
      "metadata": {
        "id": "l8oUskRIDRRK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "ft = fasttext.load_model('./data.nosync/cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 602,
      "metadata": {
        "id": "T6YnJEK8DWZo"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def convert_to_embedding(data):\n",
        "  final_data = []\n",
        "  for document in data: \n",
        "    embedding = []\n",
        "    count = 0\n",
        "    if (type(document) != list): document = ast.literal_eval(document)\n",
        "    for word in document:\n",
        "      word_embedding = ft.get_word_vector(word)\n",
        "      if len(embedding)==0: \n",
        "        embedding = word_embedding \n",
        "      else: embedding += word_embedding\n",
        "      count += 1\n",
        "    embedding /= count\n",
        "    final_data.append(embedding.tolist())\n",
        "  return pd.Series(final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df['pretrained_fasttext'] = convert_to_embedding(corpus_df['clean_summary'])\n",
        "corpus_df['pretrained_fasttext_title'] = convert_to_embedding(corpus_df['clean_title'])\n",
        "corpus_df['pretrained_fasttext_title&summary'] = convert_to_embedding(corpus_df['clean_title&summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 607,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df.to_csv('./cleaned_data_fasttext_full.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvHLskhkAinE"
      },
      "source": [
        "## Fastext Pretrained CompressedFastText"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta forma de implementación quedó descartada al ser una version de fasttext muy ligera y dejar fuera representaciones de palabras muy importantes como 5g, 4g o Wifi. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lL3YwdC9Mql",
        "outputId": "25dd9165-d502-4cf3-885c-f6384ce3a3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.2.0\n",
            "  Using cached gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim==4.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_lwSHn7eTs1",
        "outputId": "eac39477-e79a-43a0-fcc6-0a499b44a73d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting compress-fasttext\n",
            "  Downloading compress-fasttext-0.1.3.tar.gz (14 kB)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from compress-fasttext) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from compress-fasttext) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=4.0.0->compress-fasttext) (1.7.3)\n",
            "Building wheels for collected packages: compress-fasttext\n",
            "  Building wheel for compress-fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compress-fasttext: filename=compress_fasttext-0.1.3-py3-none-any.whl size=14601 sha256=7c8a3b669000f1e464cea3cd5842510d7eacca870dc031829cac6a72a649b339\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/63/9f/39db0410175167cee5eeae4fde2405d957cd05c1d8811a51cf\n",
            "Successfully built compress-fasttext\n",
            "Installing collected packages: compress-fasttext\n",
            "Successfully installed compress-fasttext-0.1.3\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import compress_fasttext\n",
        "except ModuleNotFoundError:\n",
        "  ! pip install compress-fasttext\n",
        "  import compress_fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdbFO8k22jQp"
      },
      "outputs": [],
      "source": [
        "#with open(\"cleaned_summary.txt\", 'w', encoding='utf-8') as fout:\n",
        "#  for el in corpus_df['clean_summary'].values.tolist():\n",
        "#    fout.write(' '.join(el) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4mYcH9B9CAP"
      },
      "outputs": [],
      "source": [
        "# fastTextPre = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\n",
        "#     'https://github.com/avidale/compress-fasttext/releases/download/v0.0.4/cc.en.300.compressed.bin'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqiB7rN2EQaH"
      },
      "source": [
        "## Fastext Custom Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 608,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# with open(\"cleaned_title&summary.txt\", 'w', encoding='utf-8') as fout:\n",
        "#   for el in corpus_df['clean_title&summary'].values.tolist():\n",
        "#     fout.write(' '.join(el) + '\\n')\n",
        "\n",
        "# Esta sentencia es necesaria una vez para entrenar fast text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pKIYkSafDGtd",
        "outputId": "4e504460-a64d-4c47-e648-1e81ddf8c6e8"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "import copy\n",
        "\n",
        "class IterableCorpus_fromfile:\n",
        "    def __init__(self, filename):\n",
        "        self.__filename = filename\n",
        "    def __iter__(self):\n",
        "        for line in open(self.__filename):\n",
        "            yield line.lower().split()\n",
        "\n",
        "model_fasttext = FastText(sentences=IterableCorpus_fromfile(\"cleaned_summary.txt\"), vector_size=300, min_count = 5, window=5, workers=4, seed=42, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "U_VtbaeBXLiQ",
        "outputId": "6c7404f0-2e55-4810-e072-d89d081268d8"
      },
      "outputs": [],
      "source": [
        "model_fasttext.save(\"model_fastText_cleansummary.model\")\n",
        "word_vectors = model_fasttext.wv\n",
        "word_vectors.save(\"model_fastText_cleansummary.wordvectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_jceRqY8W781",
        "outputId": "0bc66aad-4f7d-4403-8950-066e92c1a41b"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fastText_wv = KeyedVectors.load(\"model_fastText_cleansummary.wordvectors\", mmap='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_embedding_ff(data):\n",
        "  final_data = []\n",
        "  for document in data: \n",
        "    embedding = []\n",
        "    count = 0\n",
        "    if (type(document) != list): document = ast.literal_eval(document)\n",
        "    for word in document:\n",
        "      word_embedding = fastText_wv.get_vector(word).copy()\n",
        "      word_embedding.setflags(write=True)\n",
        "      if len(embedding)==0: \n",
        "        embedding = word_embedding \n",
        "      else: embedding += word_embedding\n",
        "      count += 1\n",
        "    embedding /= count\n",
        "    final_data.append(embedding.tolist())\n",
        "    del embedding\n",
        "  return pd.Series(final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df['custom_trained_fasttext'] = convert_to_embedding_ff(corpus_df['clean_summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 610,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_fasttext_ts = FastText(sentences=IterableCorpus_fromfile(\"cleaned_title&summary.txt\"), vector_size=300, min_count = 5, window=5, workers=4, seed=42, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 611,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_fasttext.save(\"model_fastText_cleantitle&summary.model\")\n",
        "word_vectors = model_fasttext.wv\n",
        "word_vectors.save(\"model_fastText_cleantitle&summary.wordvectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fastText_wv = KeyedVectors.load(\"model_fastText_cleantitle&summary.wordvectors\", mmap='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df['custom_trained_fasttext_title'] = convert_to_embedding_ff(corpus_df['clean_title'])\n",
        "corpus_df['custom_trained_fasttext_title&summary'] = convert_to_embedding_ff(corpus_df['clean_title&summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df.to_csv('./cleaned_data_fasttext_all.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "euI9oX_gEcR-"
      },
      "source": [
        "# MLP Single Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df = pd.read_csv('./cleaned_data_fasttext_all.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_df['pretrained_fasttext'] = corpus_df['pretrained_fasttext'].apply(ast.literal_eval)\n",
        "corpus_df['custom_trained_fasttext'] = corpus_df['custom_trained_fasttext'].apply(ast.literal_eval)\n",
        "corpus_df['pretrained_fasttext_title'] = corpus_df['pretrained_fasttext_title'].apply(ast.literal_eval)\n",
        "corpus_df['pretrained_fasttext_title&summary'] = corpus_df['pretrained_fasttext_title&summary'].apply(ast.literal_eval)\n",
        "corpus_df['custom_trained_fasttext_title'] = corpus_df['custom_trained_fasttext_title'].apply(ast.literal_eval)\n",
        "corpus_df['custom_trained_fasttext_title&summary'] = corpus_df['custom_trained_fasttext_title&summary'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "torch.set_printoptions(precision=10)\n",
        "import ast\n",
        "import time\n",
        "from tabulate import tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0jzX0rchhnxI"
      },
      "outputs": [],
      "source": [
        "def themenumber_to_vector(number):\n",
        "    tensor = torch.zeros(6)\n",
        "    if number == 21: tensor[0] = 1\n",
        "    elif number == 23: tensor[1] = 1\n",
        "    elif number == 27: tensor[2] = 1\n",
        "    elif number == 25: tensor[3] = 1\n",
        "    elif number == 29: tensor[4] = 1\n",
        "    elif number == 31: tensor[5] = 1\n",
        "    return tensor\n",
        "\n",
        "def position_to_number(position):\n",
        "  if position == 0: return 21\n",
        "  elif position == 1: return 23\n",
        "  elif position == 2: return 27\n",
        "  elif position == 3: return 25\n",
        "  elif position == 4: return 29 \n",
        "  elif position == 5: return 31\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zVHkmoPtfHqw"
      },
      "outputs": [],
      "source": [
        "class MLP_main(nn.Module):    \n",
        "\n",
        "  def __init__(self,dimx, hidden_dim, nlabels, dropout_prob):\n",
        "      super().__init__()\n",
        "\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.fc_layers = nn.ModuleList()\n",
        "      self.fc_layers.append(nn.Linear(dimx,hidden_dim[0]))\n",
        "      for index, value in enumerate(hidden_dim[1:]):\n",
        "        self.fc_layers.append(nn.Linear(hidden_dim[index], hidden_dim[index + 1]))\n",
        "      self.fc_layers.append(nn.Linear(hidden_dim[-1],nlabels))\n",
        "      self._parameters\n",
        "      self.relu = nn.ReLU()\n",
        "      self.logsoftmax = nn.Softmax(dim = 0)\n",
        "      \n",
        "      self.dropout = nn.Dropout(p = dropout_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "        for layer in self.fc_layers[:-1]:\n",
        "          x = layer(x)\n",
        "          x = self.relu(x)\n",
        "          x = self.dropout(x)\n",
        "        x = self.fc_layers[-1](x)\n",
        "        x = self.logsoftmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ym8739C3kRI2"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience, verbose, path ,delta=0.001 ):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose: print('Validation has reduced from {} to {}. Saving model ...'.format(self.val_loss_min,val_loss))\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rZbT-BAwfbRS"
      },
      "outputs": [],
      "source": [
        "class MLP(MLP_main):\n",
        "  \n",
        "  def __init__(self,dimx, hidden_dim, nlabels,epochs,dropout_prob, name, lr = 0.001):\n",
        "    super().__init__(dimx, hidden_dim, nlabels, dropout_prob)\n",
        "\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr\n",
        "    self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "    self.criterion = nn.CrossEntropyLoss() \n",
        "    self.loss_during_training = []\n",
        "    self.valid_loss_during_training = []\n",
        "    self.name = name + str(self.hidden_dim) + '_' + str(round(time.time()))\n",
        "    self.path = ''\n",
        "    self.last_epoch = -1\n",
        "\n",
        "  def trainloop(self, train_vectors, train_labels, valid_vectors, valid_labels, patience = 20):\n",
        "\n",
        "    self.path = './best_checkpoint_' + self.name + '.pt'\n",
        "    early_stopping = EarlyStopping(patience = patience, verbose = True ,path = self.path)\n",
        "    print('Training: {}'.format(self.name))\n",
        "    \n",
        "    for e in range(0,int(self.epochs)):\n",
        "      self.last_epoch = e + 1\n",
        "      running_loss = 0.\n",
        "\n",
        "      self.train()\n",
        "\n",
        "      classes_predicted = []\n",
        "\n",
        "      for summary, label in zip(train_vectors, train_labels):\n",
        "\n",
        "        self.optim.zero_grad()\n",
        "       \n",
        "        output = self.forward(summary)\n",
        "        class_predited = position_to_number(np.argmax(output.detach().numpy()))\n",
        "        if class_predited not in classes_predicted: classes_predicted.append(class_predited)\n",
        "        # print('Vector predicted{}, realclass {}, real vector{}'.format(output,label,themenumber_to_vector(label)))\n",
        "        loss = self.criterion(output,themenumber_to_vector(label))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.optim.step()\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "\n",
        "      self.loss_during_training.append(running_loss/len(train_vectors))\n",
        "\n",
        "      self.compute_validation_loss(valid_vectors, valid_labels)\n",
        "\n",
        "      if(True):\n",
        "\n",
        "          print(\"Training loss after {}  epochs: {}, classes {}\".format(e,self.loss_during_training[-1], classes_predicted))\n",
        "          print(\"Validation loss after {} epochs: {}\".format(e,self.valid_loss_during_training[-1]))\n",
        "\n",
        "      early_stopping(self.valid_loss_during_training[-1],self)\n",
        "      if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        print(\"Loading best epoch\")\n",
        "        self.load_state_dict(torch.load(self.path))\n",
        "        break\n",
        "              \n",
        "  def compute_validation_loss(self,valid_vectors, valid_labels):\n",
        "      total_length = 0\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "\n",
        "        validation_loss = 0.\n",
        "\n",
        "        for sumary, label in zip(valid_vectors, valid_labels):\n",
        "        \n",
        "          output = self.forward(sumary)\n",
        "\n",
        "          loss = self.criterion(output,themenumber_to_vector(label))\n",
        "          \n",
        "          validation_loss += loss.item()\n",
        "\n",
        "        self.valid_loss_during_training.append(validation_loss/len(valid_vectors))\n",
        "  \n",
        "\n",
        "  def evaluate(self,test_vectors, test_labels, test_rest_labels):\n",
        "        # print('Evaluating: {} '.format(self.path))\n",
        "        accuracy = 0\n",
        "        real_accuracy = 0\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "          classes_predicted = []\n",
        "          for sumary, label, rest_labels in zip(test_vectors,test_labels, test_rest_labels):\n",
        "            logprobs = self.forward(sumary)\n",
        "            # print('probs {}'.format(logprobs))\n",
        "            class_predicted = position_to_number(np.argmax(logprobs))\n",
        "            # print('class predicted {}, real_class {}, rest labels {}'.format(class_predicted,label, rest_labels))\n",
        "            if class_predicted not in classes_predicted: classes_predicted.append(class_predicted)\n",
        "            if label == class_predicted: \n",
        "              accuracy += 1\n",
        "              real_accuracy += 1\n",
        "            elif class_predicted in ast.literal_eval(rest_labels): \n",
        "              real_accuracy +=1\n",
        "          return accuracy/len(test_vectors), real_accuracy/len(test_vectors), classes_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = corpus_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "main_theme\n",
              "23    12523\n",
              "25     6624\n",
              "29     5962\n",
              "21     5143\n",
              "31     1154\n",
              "27      646\n",
              "Name: projectID, dtype: int64"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.groupby('main_theme').count().sort_values(['projectID'], ascending=False)['projectID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "rest_themes\n",
              "[]          15050\n",
              "[23]         4980\n",
              "[25]         3138\n",
              "[21]         1810\n",
              "[29]         1620\n",
              "[31]          724\n",
              "[23, 25]      550\n",
              "[27]          520\n",
              "[25, 23]      444\n",
              "[23, 29]      290\n",
              "Name: projectID, dtype: int64"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.groupby('rest_themes').count().sort_values(['projectID'], ascending=False).head(10)['projectID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, validate, test = np.split(data.sample(frac=1, random_state=42), [int(.7*len(data)), int(.85*len(data))])\n",
        "train.reset_index(drop=True,inplace=True)\n",
        "validate.reset_index(drop=True,inplace=True)\n",
        "test.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "main_theme\n",
              "23    8779\n",
              "25    4619\n",
              "29    4134\n",
              "21    3623\n",
              "31     813\n",
              "27     468\n",
              "Name: projectID, dtype: int64"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.groupby('main_theme').count().sort_values(['projectID'], ascending=False).head(10)['projectID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "c23 = train[train['main_theme'] == 23]\n",
        "c25 = train[train['main_theme'] == 25]\n",
        "c29 = train[train['main_theme'] == 29]\n",
        "c21 = train[train['main_theme'] == 21]\n",
        "c31 = train[train['main_theme'] == 31]\n",
        "c27 = train[train['main_theme'] == 27]\n",
        "\n",
        "maxvalue = len(c27)\n",
        "uc23 = c23.sample(maxvalue)\n",
        "uc25 = c25.sample(maxvalue)\n",
        "uc29 = c29.sample(maxvalue)\n",
        "uc21 = c21.sample(maxvalue)\n",
        "uc31 = c31.sample(maxvalue)\n",
        "\n",
        "minvalue = int(len(c23) * 0.8)\n",
        "oc23 = c23.sample(minvalue)\n",
        "oc25 = c25.sample(minvalue, replace=True)\n",
        "oc29 = c29.sample(minvalue, replace=True)\n",
        "oc21 = c21.sample(minvalue, replace=True)\n",
        "oc31 = c31.sample(minvalue, replace=True)\n",
        "oc27 = c27.sample(minvalue, replace=True)\n",
        "\n",
        "undersampled_train = pd.concat([uc23,uc25,uc29,uc21,uc31,c27])\n",
        "undersampled_train.reset_index(drop = True,inplace = True)\n",
        "oversampled_train = pd.concat([oc23,oc25,oc29,oc21,oc31,oc27])\n",
        "\n",
        "\n",
        "undersampled_train = undersampled_train.sample(frac = 1)\n",
        "undersampled_train.reset_index(inplace=True,drop=True)\n",
        "oversampled_train = oversampled_train.sample(frac = 1)\n",
        "oversampled_train.reset_index(inplace=True,drop=True)\n",
        "train.reset_index(inplace=True,drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "main_theme\n",
              "21    468\n",
              "23    468\n",
              "25    468\n",
              "27    468\n",
              "29    468\n",
              "31    468\n",
              "Name: projectID, dtype: int64"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "undersampled_train.groupby('main_theme').count().sort_values(['projectID'], ascending=False)['projectID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "main_theme\n",
              "21    7023\n",
              "23    7023\n",
              "25    7023\n",
              "27    7023\n",
              "29    7023\n",
              "31    7023\n",
              "Name: projectID, dtype: int64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oversampled_train.groupby('main_theme').count().sort_values(['projectID'], ascending=False)['projectID']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Just Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-d4f8d9b76fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_fb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_fasttext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_trained_fasttext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_theme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_under_fb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mundersampled_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_fasttext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "train_fb = torch.tensor(train.pretrained_fasttext)\n",
        "train_custom = torch.tensor(train.custom_trained_fasttext)\n",
        "train_labels = torch.tensor(train.main_theme)\n",
        "\n",
        "train_under_fb = torch.tensor(undersampled_train.pretrained_fasttext)\n",
        "train_under_custom = torch.tensor(undersampled_train.custom_trained_fasttext)\n",
        "train_under_labels = torch.tensor(undersampled_train.main_theme)\n",
        "\n",
        "train_over_fb = torch.tensor(oversampled_train.pretrained_fasttext)\n",
        "train_over_custom = torch.tensor(oversampled_train.custom_trained_fasttext)\n",
        "train_over_labels = torch.tensor(oversampled_train.main_theme)\n",
        "\n",
        "validate_fb = torch.tensor(validate.pretrained_fasttext)\n",
        "validate_custom = torch.tensor(validate.custom_trained_fasttext)\n",
        "validate_labels = torch.tensor(validate.main_theme)\n",
        "\n",
        "test_fb = torch.tensor(test.pretrained_fasttext)\n",
        "test_custom = torch.tensor(test.custom_trained_fasttext)\n",
        "test_labels = torch.tensor(test.main_theme)\n",
        "test_rest_labels = test.rest_themes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {
        "id": "eHo8PuP_k1e5"
      },
      "outputs": [],
      "source": [
        "pretrained_fasttext_MLP_u = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_under_')\n",
        "pretrained_fasttext_MLP_o = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_over_')\n",
        "pretrained_fasttext_MLP = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_')\n",
        "custom_fasttext_MLP_u = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_under_')\n",
        "custom_fasttext_MLP_o = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_over_' )\n",
        "custom_fasttext_MLP = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_fasttext_MLP_u.trainloop(train_under_fb,train_under_labels,validate_fb,validate_labels)\n",
        "pretrained_fasttext_MLP_o.trainloop(train_over_fb,train_over_labels,validate_fb,validate_labels)\n",
        "pretrained_fasttext_MLP.trainloop(train_fb,train_labels,validate_fb,validate_labels)\n",
        "custom_fasttext_MLP_u.trainloop(train_under_custom, train_under_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP_o.trainloop(train_over_custom, train_over_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP.trainloop(train_custom, train_labels, validate_custom, validate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_pre_u, real_acc_pre_u, pred_pre_u = pretrained_fasttext_MLP_u.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre_o, real_acc_pre_o, pred_pre_o = pretrained_fasttext_MLP_o.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre, real_acc_pre, pred_pre = pretrained_fasttext_MLP.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_cust_u, real_acc_cust_u, pred_cust_u = custom_fasttext_MLP_u.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust_o, real_acc_cust_o, pred_cust_o = custom_fasttext_MLP_o.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust, real_acc_cust, pred_cust = custom_fasttext_MLP.evaluate(test_custom,test_labels, test_rest_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [],
      "source": [
        "table = [\n",
        "    ['Model Name', 'Accuracy','Real Accuracy','Predited Classes','Last Epoch','Best Epoch Train','Best Epoch Validate'],\n",
        "    [pretrained_fasttext_MLP_u.name,acc_pre_u,real_acc_pre_u,pred_pre_u,pretrained_fasttext_MLP_u.last_epoch,min(pretrained_fasttext_MLP_u.loss_during_training),min(pretrained_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP_o.name,acc_pre_o,real_acc_pre_o,pred_pre_o,pretrained_fasttext_MLP_o.last_epoch,min(pretrained_fasttext_MLP_o.loss_during_training),min(pretrained_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP.name,acc_pre,real_acc_pre,pred_pre,pretrained_fasttext_MLP.last_epoch,min(pretrained_fasttext_MLP.loss_during_training),min(pretrained_fasttext_MLP.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_u.name,acc_cust_u,real_acc_cust_u,pred_cust_u,custom_fasttext_MLP_u.last_epoch,min(custom_fasttext_MLP_u.loss_during_training),min(custom_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_o.name,acc_cust_o,real_acc_cust_o,pred_cust_o,custom_fasttext_MLP_o.last_epoch,min(custom_fasttext_MLP_o.loss_during_training),min(custom_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP.name,acc_cust,real_acc_cust,pred_cust,custom_fasttext_MLP.last_epoch,min(custom_fasttext_MLP.loss_during_training),min(custom_fasttext_MLP.valid_loss_during_training)]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════════════════════════════════════════════╤════════════╤═════════════════╤══════════════════════════╤══════════════╤════════════════════╤═══════════════════════╕\n",
            "│ Model Name                                 │   Accuracy │   Real Accuracy │ Predited Classes         │   Last Epoch │   Best Epoch Train │   Best Epoch Validate │\n",
            "╞════════════════════════════════════════════╪════════════╪═════════════════╪══════════════════════════╪══════════════╪════════════════════╪═══════════════════════╡\n",
            "│ pretrained_fast_under_[100, 50]_1672930517 │   0.576123 │        0.809692 │ [21, 23, 25, 29, 31, 27] │           39 │            1.31173 │               1.4513  │\n",
            "├────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_fast_over_[100, 50]_1672930517  │   0.603993 │        0.832363 │ [21, 23, 29, 25, 31, 27] │           66 │            1.31854 │               1.42864 │\n",
            "├────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_fast_[100, 50]_1672919850       │   0.616057 │        0.832987 │ [21, 23, 25, 29]         │           25 │            1.40534 │               1.40688 │\n",
            "├────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_under_[100, 50]_1672930517     │   0.590266 │        0.827995 │ [21, 25, 29, 23, 31, 27] │           42 │            1.36222 │               1.4464  │\n",
            "├────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_over_[100, 50]_1672930517      │   0.612521 │        0.845466 │ [21, 29, 25, 23, 31, 27] │           29 │            1.38739 │               1.43254 │\n",
            "├────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_[100, 50]_1672930517           │   0.618344 │        0.839018 │ [21, 23, 29, 25]         │           33 │            1.42439 │               1.40778 │\n",
            "╘════════════════════════════════════════════╧════════════╧═════════════════╧══════════════════════════╧══════════════╧════════════════════╧═══════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_summary_results = pd.DataFrame(data = table[1:], columns = table[0])\n",
        "text_summary_results.to_csv('best_results_text_summary.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Title And Summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Toguether (one vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 642,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_fb = torch.tensor(train['pretrained_fasttext_title&summary'])\n",
        "train_custom = torch.tensor(train['custom_trained_fasttext_title&summary'])\n",
        "train_labels = torch.tensor(train.main_theme)\n",
        "\n",
        "train_under_fb = torch.tensor(undersampled_train['pretrained_fasttext_title&summary'])\n",
        "train_under_custom = torch.tensor(undersampled_train['custom_trained_fasttext_title&summary'])\n",
        "train_under_labels = torch.tensor(undersampled_train.main_theme)\n",
        "\n",
        "train_over_fb = torch.tensor(oversampled_train['pretrained_fasttext_title&summary'])\n",
        "train_over_custom = torch.tensor(oversampled_train['custom_trained_fasttext_title&summary'])\n",
        "train_over_labels = torch.tensor(oversampled_train.main_theme)\n",
        "\n",
        "validate_fb = torch.tensor(validate['pretrained_fasttext_title&summary'])\n",
        "validate_custom = torch.tensor(validate['custom_trained_fasttext_title&summary'])\n",
        "validate_labels = torch.tensor(validate.main_theme)\n",
        "\n",
        "test_fb = torch.tensor(test['pretrained_fasttext_title&summary'])\n",
        "test_custom = torch.tensor(test['custom_trained_fasttext_title&summary'])\n",
        "test_labels = torch.tensor(test.main_theme)\n",
        "test_rest_labels = test.rest_themes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 646,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretrained_fasttext_MLP_u = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_tsone_under_')\n",
        "pretrained_fasttext_MLP_o = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_tsone_over_')\n",
        "# pretrained_fasttext_MLP = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_tsone_fast_')\n",
        "custom_fasttext_MLP_u = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tsone_under_')\n",
        "custom_fasttext_MLP_o = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tsone_over_' )\n",
        "custom_fasttext_MLP = MLP(dimx = 300, hidden_dim = [100,50], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tsone_' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 647,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: pretrained_fast_tsone_under_[100, 50]_1673102749\n",
            "Training loss after 0  epochs: 1.675601868165864, classes [23, 27, 31, 25, 21, 29]\n",
            "Validation loss after 0 epochs: 1.7140661981111756\n",
            "Validation has reduced from inf to 1.7140661981111756. Saving model ...\n",
            "Training loss after 1  epochs: 1.5453554285727336, classes [27, 31, 29, 21, 25, 23]\n",
            "Validation loss after 1 epochs: 1.626965512972505\n",
            "Validation has reduced from 1.7140661981111756 to 1.626965512972505. Saving model ...\n",
            "Training loss after 2  epochs: 1.5038283423141197, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 2 epochs: 1.59631246008512\n",
            "Validation has reduced from 1.626965512972505 to 1.59631246008512. Saving model ...\n",
            "Training loss after 3  epochs: 1.4781497128074326, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 3 epochs: 1.5405810702709906\n",
            "Validation has reduced from 1.59631246008512 to 1.5405810702709906. Saving model ...\n",
            "Training loss after 4  epochs: 1.4607779249081925, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 4 epochs: 1.5511684188876493\n",
            "Training loss after 5  epochs: 1.4452932571244037, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 5 epochs: 1.5337361723382936\n",
            "Validation has reduced from 1.5405810702709906 to 1.5337361723382936. Saving model ...\n",
            "Training loss after 6  epochs: 1.4351373999207109, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 6 epochs: 1.5395879754905097\n",
            "Training loss after 7  epochs: 1.4233496921439457, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 7 epochs: 1.5127014250257447\n",
            "Validation has reduced from 1.5337361723382936 to 1.5127014250257447. Saving model ...\n",
            "Training loss after 8  epochs: 1.419512781619686, classes [27, 31, 25, 29, 21, 23]\n",
            "Validation loss after 8 epochs: 1.5224278064912646\n",
            "Training loss after 9  epochs: 1.4073763423495822, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 9 epochs: 1.5170019009644895\n",
            "Training loss after 10  epochs: 1.404599545263497, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 10 epochs: 1.5009759991567662\n",
            "Validation has reduced from 1.5127014250257447 to 1.5009759991567662. Saving model ...\n",
            "Training loss after 11  epochs: 1.3961711144685065, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 11 epochs: 1.5166465393120359\n",
            "Training loss after 12  epochs: 1.3916428481836265, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 12 epochs: 1.5242217326917982\n",
            "Training loss after 13  epochs: 1.384465119904942, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 13 epochs: 1.5126098183149506\n",
            "Training loss after 14  epochs: 1.3790753501602726, classes [27, 31, 25, 29, 21, 23]\n",
            "Validation loss after 14 epochs: 1.5109047207380095\n",
            "Training loss after 15  epochs: 1.3783581884328457, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 15 epochs: 1.5228990569785112\n",
            "Training loss after 16  epochs: 1.3746580700589042, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 16 epochs: 1.501016469198138\n",
            "Training loss after 17  epochs: 1.3755874071705376, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 17 epochs: 1.4891761781124029\n",
            "Validation has reduced from 1.5009759991567662 to 1.4891761781124029. Saving model ...\n",
            "Training loss after 18  epochs: 1.3686134212377064, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 18 epochs: 1.500866979583726\n",
            "Training loss after 19  epochs: 1.3620225888201993, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 19 epochs: 1.5220017840778965\n",
            "Training loss after 20  epochs: 1.3579318049201938, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 20 epochs: 1.5051231856849943\n",
            "Training loss after 21  epochs: 1.3584562748500764, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 21 epochs: 1.470531524442595\n",
            "Validation has reduced from 1.4891761781124029 to 1.470531524442595. Saving model ...\n",
            "Training loss after 22  epochs: 1.35237839414693, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 22 epochs: 1.5192842952621954\n",
            "Training loss after 23  epochs: 1.3536561997952286, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 23 epochs: 1.4817906027030627\n",
            "Training loss after 24  epochs: 1.3457326082847056, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 24 epochs: 1.4872818087133115\n",
            "Training loss after 25  epochs: 1.3477664239216394, classes [27, 31, 29, 23, 25, 21]\n",
            "Validation loss after 25 epochs: 1.4772519574139558\n",
            "Training loss after 26  epochs: 1.3448728478786953, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 26 epochs: 1.492181485261576\n",
            "Training loss after 27  epochs: 1.3404411373824476, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 27 epochs: 1.4878960815870426\n",
            "Training loss after 28  epochs: 1.3459257005456506, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 28 epochs: 1.4853190326700592\n",
            "Training loss after 29  epochs: 1.3385948832133556, classes [27, 31, 23, 29, 25, 21]\n",
            "Validation loss after 29 epochs: 1.513808649808119\n",
            "Training loss after 30  epochs: 1.3353596678231856, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 30 epochs: 1.504604727674443\n",
            "Training loss after 31  epochs: 1.331708001275348, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 31 epochs: 1.506604436902754\n",
            "Training loss after 32  epochs: 1.3375105178169375, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 32 epochs: 1.4943770124590932\n",
            "Training loss after 33  epochs: 1.3305331525045243, classes [27, 31, 29, 23, 25, 21]\n",
            "Validation loss after 33 epochs: 1.5047615968843864\n",
            "Training loss after 34  epochs: 1.330843180927456, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 34 epochs: 1.4992570241854313\n",
            "Training loss after 35  epochs: 1.335702098927267, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 35 epochs: 1.4966236057128763\n",
            "Training loss after 36  epochs: 1.3254392034562565, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 36 epochs: 1.486880615948639\n",
            "Training loss after 37  epochs: 1.3260768034689108, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 37 epochs: 1.4935840511827818\n",
            "Training loss after 38  epochs: 1.324339803467449, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 38 epochs: 1.489145838505218\n",
            "Training loss after 39  epochs: 1.3195272335978994, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 39 epochs: 1.4905792536483629\n",
            "Training loss after 40  epochs: 1.3231233920466865, classes [27, 31, 23, 29, 25, 21]\n",
            "Validation loss after 40 epochs: 1.4921284649465325\n",
            "Training loss after 41  epochs: 1.31812260403932, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 41 epochs: 1.486167238030378\n",
            "Early stopping\n",
            "Loading best epoch\n",
            "Training: pretrained_fast_tsone_over_[100, 50]_1673102749\n",
            "Training loss after 0  epochs: 1.4658822905435092, classes [29, 23, 31, 21, 25, 27]\n",
            "Validation loss after 0 epochs: 1.502122035687061\n",
            "Validation has reduced from inf to 1.502122035687061. Saving model ...\n",
            "Training loss after 1  epochs: 1.4034715935582933, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 1 epochs: 1.4856796453082424\n",
            "Validation has reduced from 1.502122035687061 to 1.4856796453082424. Saving model ...\n",
            "Training loss after 2  epochs: 1.3939353072613292, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 2 epochs: 1.4698043894301636\n",
            "Validation has reduced from 1.4856796453082424 to 1.4698043894301636. Saving model ...\n",
            "Training loss after 3  epochs: 1.3840124232269182, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 3 epochs: 1.4931168266124217\n",
            "Training loss after 4  epochs: 1.3802886130553023, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 4 epochs: 1.4814011554005935\n",
            "Training loss after 5  epochs: 1.3764602711183853, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 5 epochs: 1.4865043807942142\n",
            "Training loss after 6  epochs: 1.3710039188692218, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 6 epochs: 1.467437952608118\n",
            "Validation has reduced from 1.4698043894301636 to 1.467437952608118. Saving model ...\n",
            "Training loss after 7  epochs: 1.3683115327933668, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 7 epochs: 1.4616362925823436\n",
            "Validation has reduced from 1.467437952608118 to 1.4616362925823436. Saving model ...\n",
            "Training loss after 8  epochs: 1.3690760329495157, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 8 epochs: 1.455882046266324\n",
            "Validation has reduced from 1.4616362925823436 to 1.455882046266324. Saving model ...\n",
            "Training loss after 9  epochs: 1.363587778061805, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 9 epochs: 1.4645594600929\n",
            "Training loss after 10  epochs: 1.3604110695350617, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 10 epochs: 1.4699899141681372\n",
            "Training loss after 11  epochs: 1.3606956806501522, classes [27, 21, 23, 25, 31, 29]\n",
            "Validation loss after 11 epochs: 1.4468753271412333\n",
            "Validation has reduced from 1.455882046266324 to 1.4468753271412333. Saving model ...\n",
            "Training loss after 12  epochs: 1.359021444116763, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 12 epochs: 1.4719565365209357\n",
            "Training loss after 13  epochs: 1.3573155242477102, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 13 epochs: 1.4552850345902753\n",
            "Training loss after 14  epochs: 1.352667393252226, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 14 epochs: 1.4547095010165565\n",
            "Training loss after 15  epochs: 1.3534594090075762, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 15 epochs: 1.4674407430674987\n",
            "Training loss after 16  epochs: 1.352864805717021, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 16 epochs: 1.460153889958553\n",
            "Training loss after 17  epochs: 1.3511775235405759, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 17 epochs: 1.4718123684558218\n",
            "Training loss after 18  epochs: 1.3502396917704123, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 18 epochs: 1.454245840501468\n",
            "Training loss after 19  epochs: 1.3510038292149418, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 19 epochs: 1.458674401292785\n",
            "Training loss after 20  epochs: 1.3473998923493844, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 20 epochs: 1.458687482380034\n",
            "Training loss after 21  epochs: 1.3475668032313395, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 21 epochs: 1.4621248737647015\n",
            "Training loss after 22  epochs: 1.3430937778186467, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 22 epochs: 1.4593911570885811\n",
            "Training loss after 23  epochs: 1.3422136231623896, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 23 epochs: 1.4715833331205683\n",
            "Training loss after 24  epochs: 1.343350172501148, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 24 epochs: 1.476909279922479\n",
            "Training loss after 25  epochs: 1.3440960397089579, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 25 epochs: 1.4509803997399209\n",
            "Training loss after 26  epochs: 1.3445048640153299, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 26 epochs: 1.460810680010553\n",
            "Training loss after 27  epochs: 1.3431861941371623, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 27 epochs: 1.446607055362568\n",
            "Training loss after 28  epochs: 1.3415710204853568, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 28 epochs: 1.4469850750364202\n",
            "Training loss after 29  epochs: 1.3409731372362683, classes [27, 21, 23, 25, 31, 29]\n",
            "Validation loss after 29 epochs: 1.4575502371381404\n",
            "Training loss after 30  epochs: 1.3400365607168956, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 30 epochs: 1.469321081176177\n",
            "Training loss after 31  epochs: 1.3408989582982218, classes [25, 21, 23, 27, 31, 29]\n",
            "Validation loss after 31 epochs: 1.4563148844856986\n",
            "Early stopping\n",
            "Loading best epoch\n",
            "Training: custom_fast_tsone_under_[100, 50]_1673102749\n",
            "Training loss after 0  epochs: 1.6152075028147792, classes [21, 31, 25, 27, 23, 29]\n",
            "Validation loss after 0 epochs: 1.6242432690251885\n",
            "Validation has reduced from inf to 1.6242432690251885. Saving model ...\n",
            "Training loss after 1  epochs: 1.5003365805091342, classes [27, 31, 25, 21, 23, 29]\n",
            "Validation loss after 1 epochs: 1.553282043997341\n",
            "Validation has reduced from 1.6242432690251885 to 1.553282043997341. Saving model ...\n",
            "Training loss after 2  epochs: 1.4673705132004202, classes [27, 31, 25, 29, 21, 23]\n",
            "Validation loss after 2 epochs: 1.5224445946948502\n",
            "Validation has reduced from 1.553282043997341 to 1.5224445946948502. Saving model ...\n",
            "Training loss after 3  epochs: 1.4455556303602337, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 3 epochs: 1.5160421632143504\n",
            "Validation has reduced from 1.5224445946948502 to 1.5160421632143504. Saving model ...\n",
            "Training loss after 4  epochs: 1.4401975552580635, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 4 epochs: 1.5097463840057768\n",
            "Validation has reduced from 1.5160421632143504 to 1.5097463840057768. Saving model ...\n",
            "Training loss after 5  epochs: 1.4260715342569894, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 5 epochs: 1.4858648092943103\n",
            "Validation has reduced from 1.5097463840057768 to 1.4858648092943103. Saving model ...\n",
            "Training loss after 6  epochs: 1.4238882953794594, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 6 epochs: 1.4865724125291266\n",
            "Training loss after 7  epochs: 1.4203083679037556, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 7 epochs: 1.5087754978316397\n",
            "Training loss after 8  epochs: 1.4205677074584526, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 8 epochs: 1.4925134916125042\n",
            "Training loss after 9  epochs: 1.4136325832870271, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 9 epochs: 1.5090099755866753\n",
            "Training loss after 10  epochs: 1.4086647237467969, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 10 epochs: 1.5194974592987591\n",
            "Training loss after 11  epochs: 1.4124335365162954, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 11 epochs: 1.4533354984146982\n",
            "Validation has reduced from 1.4858648092943103 to 1.4533354984146982. Saving model ...\n",
            "Training loss after 12  epochs: 1.404875737267342, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 12 epochs: 1.4593200739132188\n",
            "Training loss after 13  epochs: 1.3992437211706428, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 13 epochs: 1.4783072874024386\n",
            "Training loss after 14  epochs: 1.4099920346611245, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 14 epochs: 1.4853891801467156\n",
            "Training loss after 15  epochs: 1.3981751395002049, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 15 epochs: 1.475481418722283\n",
            "Training loss after 16  epochs: 1.3958863089566897, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 16 epochs: 1.4583807453240412\n",
            "Training loss after 17  epochs: 1.398884386715726, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 17 epochs: 1.4667229635296963\n",
            "Training loss after 18  epochs: 1.3995709775010405, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 18 epochs: 1.504670411298755\n",
            "Training loss after 19  epochs: 1.3867486263612057, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 19 epochs: 1.4753231942504892\n",
            "Training loss after 20  epochs: 1.392712957881115, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 20 epochs: 1.4474618313911551\n",
            "Validation has reduced from 1.4533354984146982 to 1.4474618313911551. Saving model ...\n",
            "Training loss after 21  epochs: 1.388256785706577, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 21 epochs: 1.4655606460055575\n",
            "Training loss after 22  epochs: 1.388831203904247, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 22 epochs: 1.503716546465672\n",
            "Training loss after 23  epochs: 1.3880795294742638, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 23 epochs: 1.468016031478685\n",
            "Training loss after 24  epochs: 1.3794927127086198, classes [27, 31, 29, 23, 25, 21]\n",
            "Validation loss after 24 epochs: 1.475876289353196\n",
            "Training loss after 25  epochs: 1.3864691235401012, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 25 epochs: 1.4875040454743507\n",
            "Training loss after 26  epochs: 1.374347704088586, classes [27, 31, 25, 29, 21, 23]\n",
            "Validation loss after 26 epochs: 1.534737494245742\n",
            "Training loss after 27  epochs: 1.3731437697369828, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 27 epochs: 1.4867833256225618\n",
            "Training loss after 28  epochs: 1.3785575918662243, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 28 epochs: 1.4979365182646895\n",
            "Training loss after 29  epochs: 1.3741458489259764, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 29 epochs: 1.492189958642009\n",
            "Training loss after 30  epochs: 1.3749855020192274, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 30 epochs: 1.454236869680108\n",
            "Training loss after 31  epochs: 1.3857870600332223, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 31 epochs: 1.5020615131456325\n",
            "Training loss after 32  epochs: 1.374824541883591, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 32 epochs: 1.5078362025407308\n",
            "Training loss after 33  epochs: 1.3762081216628055, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 33 epochs: 1.4622230738550177\n",
            "Training loss after 34  epochs: 1.3789877689462102, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 34 epochs: 1.4381889282615727\n",
            "Validation has reduced from 1.4474618313911551 to 1.4381889282615727. Saving model ...\n",
            "Training loss after 35  epochs: 1.3679924342791918, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 35 epochs: 1.483371105398394\n",
            "Training loss after 36  epochs: 1.3740490514923025, classes [27, 31, 21, 29, 25, 23]\n",
            "Validation loss after 36 epochs: 1.4794686209540597\n",
            "Training loss after 37  epochs: 1.3804981931407228, classes [27, 31, 23, 29, 25, 21]\n",
            "Validation loss after 37 epochs: 1.4852681469401583\n",
            "Training loss after 38  epochs: 1.3719437564285393, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 38 epochs: 1.4650213684198663\n",
            "Training loss after 39  epochs: 1.375285099968951, classes [27, 31, 29, 23, 25, 21]\n",
            "Validation loss after 39 epochs: 1.489266018775259\n",
            "Training loss after 40  epochs: 1.3755144844961982, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 40 epochs: 1.4608496508612212\n",
            "Training loss after 41  epochs: 1.3618733679976558, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 41 epochs: 1.4804503697523856\n",
            "Training loss after 42  epochs: 1.3748725719771153, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 42 epochs: 1.493018213901266\n",
            "Training loss after 43  epochs: 1.3731514770760496, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 43 epochs: 1.4605696732658713\n",
            "Training loss after 44  epochs: 1.364082250584904, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 44 epochs: 1.496362120483362\n",
            "Training loss after 45  epochs: 1.3702834539773456, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 45 epochs: 1.5077895699642263\n",
            "Training loss after 46  epochs: 1.3630112801437024, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 46 epochs: 1.4432865504714694\n",
            "Training loss after 47  epochs: 1.3588060087118394, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 47 epochs: 1.4913700672235346\n",
            "Training loss after 48  epochs: 1.3699771151413604, classes [27, 31, 25, 29, 23, 21]\n",
            "Validation loss after 48 epochs: 1.498264577915387\n",
            "Training loss after 49  epochs: 1.3678817749447971, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 49 epochs: 1.4800472382003576\n",
            "Training loss after 50  epochs: 1.3713827730923296, classes [27, 31, 29, 25, 21, 23]\n",
            "Validation loss after 50 epochs: 1.4626806392497111\n",
            "Training loss after 51  epochs: 1.3668211580936047, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 51 epochs: 1.491480002015482\n",
            "Training loss after 52  epochs: 1.3680571522722897, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 52 epochs: 1.4608371175961963\n",
            "Training loss after 53  epochs: 1.3557259271032789, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 53 epochs: 1.4537024062852097\n",
            "Training loss after 54  epochs: 1.3651978290318763, classes [27, 31, 29, 25, 23, 21]\n",
            "Validation loss after 54 epochs: 1.447776153410929\n",
            "Early stopping\n",
            "Loading best epoch\n",
            "Training: custom_fast_tsone_over_[100, 50]_1673102749\n",
            "Training loss after 0  epochs: 1.4476943209387545, classes [23, 25, 31, 29, 21, 27]\n",
            "Validation loss after 0 epochs: 1.5444569245303132\n",
            "Validation has reduced from inf to 1.5444569245303132. Saving model ...\n",
            "Training loss after 1  epochs: 1.4107317302765552, classes [25, 29, 23, 21, 27, 31]\n",
            "Validation loss after 1 epochs: 1.4942047005138064\n",
            "Validation has reduced from 1.5444569245303132 to 1.4942047005138064. Saving model ...\n",
            "Training loss after 2  epochs: 1.4055157933606788, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 2 epochs: 1.4930537413192073\n",
            "Validation has reduced from 1.4942047005138064 to 1.4930537413192073. Saving model ...\n",
            "Training loss after 3  epochs: 1.3994730226230516, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 3 epochs: 1.4833400023608359\n",
            "Validation has reduced from 1.4930537413192073 to 1.4833400023608359. Saving model ...\n",
            "Training loss after 4  epochs: 1.3993381490778236, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 4 epochs: 1.4475005394507963\n",
            "Validation has reduced from 1.4833400023608359 to 1.4475005394507963. Saving model ...\n",
            "Training loss after 5  epochs: 1.3953850516262876, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 5 epochs: 1.4413050622392614\n",
            "Validation has reduced from 1.4475005394507963 to 1.4413050622392614. Saving model ...\n",
            "Training loss after 6  epochs: 1.3932850854817438, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 6 epochs: 1.4467830491343672\n",
            "Training loss after 7  epochs: 1.3970579597928368, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 7 epochs: 1.4785928013618297\n",
            "Training loss after 8  epochs: 1.3991803000171215, classes [25, 21, 23, 27, 31, 29]\n",
            "Validation loss after 8 epochs: 1.4988466590990044\n",
            "Training loss after 9  epochs: 1.394673865447098, classes [25, 21, 23, 27, 31, 29]\n",
            "Validation loss after 9 epochs: 1.4431063755478915\n",
            "Training loss after 10  epochs: 1.3967811254388136, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 10 epochs: 1.4509099860258785\n",
            "Training loss after 11  epochs: 1.3977378325761312, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 11 epochs: 1.474343836629847\n",
            "Training loss after 12  epochs: 1.397129889876973, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 12 epochs: 1.4737322461882765\n",
            "Training loss after 13  epochs: 1.401830040668467, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 13 epochs: 1.4642221206882828\n",
            "Training loss after 14  epochs: 1.398157395422105, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 14 epochs: 1.492486337159517\n",
            "Training loss after 15  epochs: 1.3968615452029831, classes [25, 21, 23, 27, 31, 29]\n",
            "Validation loss after 15 epochs: 1.4434540516475274\n",
            "Training loss after 16  epochs: 1.3918942842430788, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 16 epochs: 1.4661124339218743\n",
            "Training loss after 17  epochs: 1.3963677430363373, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 17 epochs: 1.4827541714400896\n",
            "Training loss after 18  epochs: 1.395863301391091, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 18 epochs: 1.4460284267358494\n",
            "Training loss after 19  epochs: 1.3962459732450867, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 19 epochs: 1.4452945304392975\n",
            "Training loss after 20  epochs: 1.3955793708875457, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 20 epochs: 1.4878951671367477\n",
            "Training loss after 21  epochs: 1.3973909505912194, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 21 epochs: 1.478099819328543\n",
            "Training loss after 22  epochs: 1.3956483255912142, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 22 epochs: 1.4557118861875598\n",
            "Training loss after 23  epochs: 1.3979193186538152, classes [25, 21, 23, 29, 27, 31]\n",
            "Validation loss after 23 epochs: 1.5421250693859554\n",
            "Training loss after 24  epochs: 1.4009084294753147, classes [27, 21, 23, 29, 25, 31]\n",
            "Validation loss after 24 epochs: 1.4912825640098823\n",
            "Training loss after 25  epochs: 1.3958210024622522, classes [25, 29, 23, 21, 27, 31]\n",
            "Validation loss after 25 epochs: 1.4613333378923117\n",
            "Early stopping\n",
            "Loading best epoch\n",
            "Training: custom_fast_tsone_[100, 50]_1673102749\n",
            "Training loss after 0  epochs: 1.5403689749526943, classes [29, 23, 25]\n",
            "Validation loss after 0 epochs: 1.5083837851063782\n",
            "Validation has reduced from inf to 1.5083837851063782. Saving model ...\n",
            "Training loss after 1  epochs: 1.514993047189279, classes [25, 29, 23]\n",
            "Validation loss after 1 epochs: 1.496841440408083\n",
            "Validation has reduced from 1.5083837851063782 to 1.496841440408083. Saving model ...\n",
            "Training loss after 2  epochs: 1.4866802637314367, classes [23, 29, 25, 21]\n",
            "Validation loss after 2 epochs: 1.4258713969672578\n",
            "Validation has reduced from 1.496841440408083 to 1.4258713969672578. Saving model ...\n",
            "Training loss after 3  epochs: 1.4363052690649738, classes [29, 23, 25, 21]\n",
            "Validation loss after 3 epochs: 1.4187950494880486\n",
            "Validation has reduced from 1.4258713969672578 to 1.4187950494880486. Saving model ...\n",
            "Training loss after 4  epochs: 1.4322688125537495, classes [29, 23, 25, 21]\n",
            "Validation loss after 4 epochs: 1.4166246950130494\n",
            "Validation has reduced from 1.4187950494880486 to 1.4166246950130494. Saving model ...\n",
            "Training loss after 5  epochs: 1.4339237265523943, classes [29, 23, 25, 21]\n",
            "Validation loss after 5 epochs: 1.431628427841699\n",
            "Training loss after 6  epochs: 1.435490245077567, classes [29, 23, 25, 21]\n",
            "Validation loss after 6 epochs: 1.4162553977996062\n",
            "Training loss after 7  epochs: 1.4337615827322643, classes [23, 29, 25, 21]\n",
            "Validation loss after 7 epochs: 1.415885927871539\n",
            "Training loss after 8  epochs: 1.4300140512034225, classes [23, 29, 25, 21]\n",
            "Validation loss after 8 epochs: 1.4283542534872617\n",
            "Training loss after 9  epochs: 1.4314784517229124, classes [23, 29, 21, 25]\n",
            "Validation loss after 9 epochs: 1.4163958556739344\n",
            "Training loss after 10  epochs: 1.4330168647187176, classes [29, 23, 25, 21]\n",
            "Validation loss after 10 epochs: 1.4330893314122757\n",
            "Training loss after 11  epochs: 1.4243241448913757, classes [23, 29, 25, 21]\n",
            "Validation loss after 11 epochs: 1.4124009316157977\n",
            "Validation has reduced from 1.4166246950130494 to 1.4124009316157977. Saving model ...\n",
            "Training loss after 12  epochs: 1.4267491310234481, classes [29, 23, 25, 21]\n",
            "Validation loss after 12 epochs: 1.4206780948228328\n",
            "Training loss after 13  epochs: 1.4298302596808543, classes [23, 29, 25, 21]\n",
            "Validation loss after 13 epochs: 1.4362650830515609\n",
            "Training loss after 14  epochs: 1.4314073557616938, classes [25, 23, 29, 21]\n",
            "Validation loss after 14 epochs: 1.4317054903249375\n",
            "Training loss after 15  epochs: 1.4279588204820985, classes [23, 25, 29, 21]\n",
            "Validation loss after 15 epochs: 1.4162747563915126\n",
            "Training loss after 16  epochs: 1.4236959637189044, classes [23, 29, 25, 21]\n",
            "Validation loss after 16 epochs: 1.417191354164268\n",
            "Training loss after 17  epochs: 1.4268113200380577, classes [29, 23, 25, 21]\n",
            "Validation loss after 17 epochs: 1.419960225555742\n",
            "Training loss after 18  epochs: 1.425411385220201, classes [25, 29, 23, 21]\n",
            "Validation loss after 18 epochs: 1.4128736893517801\n",
            "Training loss after 19  epochs: 1.4279011089197347, classes [29, 23, 25, 21]\n",
            "Validation loss after 19 epochs: 1.419686787388289\n",
            "Training loss after 20  epochs: 1.4297327259354549, classes [29, 23, 25, 21]\n",
            "Validation loss after 20 epochs: 1.415349645766363\n",
            "Training loss after 21  epochs: 1.4278247200416188, classes [29, 23, 25, 21]\n",
            "Validation loss after 21 epochs: 1.4103590486053619\n",
            "Validation has reduced from 1.4124009316157977 to 1.4103590486053619. Saving model ...\n",
            "Training loss after 22  epochs: 1.435352571349791, classes [23, 25, 29, 21]\n",
            "Validation loss after 22 epochs: 1.4185362952124458\n",
            "Training loss after 23  epochs: 1.4264416884910531, classes [29, 23, 25, 21]\n",
            "Validation loss after 23 epochs: 1.4107203899376008\n",
            "Training loss after 24  epochs: 1.4223587226795416, classes [25, 29, 23, 21]\n",
            "Validation loss after 24 epochs: 1.4092052905214407\n",
            "Validation has reduced from 1.4103590486053619 to 1.4092052905214407. Saving model ...\n",
            "Training loss after 25  epochs: 1.428951037638879, classes [23, 29, 25, 21]\n",
            "Validation loss after 25 epochs: 1.4126067825293978\n",
            "Training loss after 26  epochs: 1.4259175196553024, classes [29, 23, 25, 21]\n",
            "Validation loss after 26 epochs: 1.4125824533216964\n",
            "Training loss after 27  epochs: 1.4237778191473802, classes [29, 23, 25, 21]\n",
            "Validation loss after 27 epochs: 1.4486457007796119\n",
            "Training loss after 28  epochs: 1.4228638185284146, classes [23, 29, 21, 25]\n",
            "Validation loss after 28 epochs: 1.4152170561514361\n",
            "Training loss after 29  epochs: 1.4206978143866122, classes [23, 29, 25, 21]\n",
            "Validation loss after 29 epochs: 1.4174729090760432\n",
            "Training loss after 30  epochs: 1.428602313194115, classes [23, 29, 25, 21]\n",
            "Validation loss after 30 epochs: 1.415362143972749\n",
            "Training loss after 31  epochs: 1.4276320242290936, classes [23, 25, 29, 21]\n",
            "Validation loss after 31 epochs: 1.434149237867203\n",
            "Training loss after 32  epochs: 1.4264391045197167, classes [23, 25, 29, 21]\n",
            "Validation loss after 32 epochs: 1.424999473669168\n",
            "Training loss after 33  epochs: 1.4202551883923473, classes [23, 29, 25, 21]\n",
            "Validation loss after 33 epochs: 1.418711787873814\n",
            "Training loss after 34  epochs: 1.4309139351705367, classes [23, 25, 29, 21]\n",
            "Validation loss after 34 epochs: 1.4395496220189998\n",
            "Training loss after 35  epochs: 1.4289048405908438, classes [29, 23, 25, 21]\n",
            "Validation loss after 35 epochs: 1.4241394201005357\n",
            "Training loss after 36  epochs: 1.4198736390246707, classes [23, 25, 29, 21]\n",
            "Validation loss after 36 epochs: 1.417894008725931\n",
            "Training loss after 37  epochs: 1.4192778385882072, classes [23, 25, 29, 21]\n",
            "Validation loss after 37 epochs: 1.417636762070973\n",
            "Training loss after 38  epochs: 1.4256275623351373, classes [23, 25, 29, 21]\n",
            "Validation loss after 38 epochs: 1.4259646210862873\n",
            "Training loss after 39  epochs: 1.4251280877706647, classes [23, 25, 29, 21]\n",
            "Validation loss after 39 epochs: 1.4227033978988248\n",
            "Training loss after 40  epochs: 1.427113765785243, classes [23, 25, 29, 21]\n",
            "Validation loss after 40 epochs: 1.4443790275671715\n",
            "Training loss after 41  epochs: 1.4360330020293213, classes [23, 25, 29, 21]\n",
            "Validation loss after 41 epochs: 1.4319452287849292\n",
            "Training loss after 42  epochs: 1.4245575614676032, classes [23, 29, 25, 21]\n",
            "Validation loss after 42 epochs: 1.4279873428398282\n",
            "Training loss after 43  epochs: 1.424690749815684, classes [29, 23, 25, 21]\n",
            "Validation loss after 43 epochs: 1.4171063428165116\n",
            "Training loss after 44  epochs: 1.4249743181374606, classes [29, 23, 25, 21]\n",
            "Validation loss after 44 epochs: 1.4177163997029703\n",
            "Early stopping\n",
            "Loading best epoch\n"
          ]
        }
      ],
      "source": [
        "pretrained_fasttext_MLP_u.trainloop(train_under_fb,train_under_labels,validate_fb,validate_labels)\n",
        "pretrained_fasttext_MLP_o.trainloop(train_over_fb,train_over_labels,validate_fb,validate_labels)\n",
        "# pretrained_fasttext_MLP.trainloop(train_fb,train_labels,validate_fb,validate_labels)\n",
        "custom_fasttext_MLP_u.trainloop(train_under_custom, train_under_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP_o.trainloop(train_over_custom, train_over_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP.trainloop(train_custom, train_labels, validate_custom, validate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 648,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "acc_pre_u, real_acc_pre_u, pred_pre_u = pretrained_fasttext_MLP_u.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre_o, real_acc_pre_o, pred_pre_o = pretrained_fasttext_MLP_o.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre, real_acc_pre, pred_pre = pretrained_fasttext_MLP.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_cust_u, real_acc_cust_u, pred_cust_u = custom_fasttext_MLP_u.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust_o, real_acc_cust_o, pred_cust_o = custom_fasttext_MLP_o.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust, real_acc_cust, pred_cust = custom_fasttext_MLP.evaluate(test_custom,test_labels, test_rest_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 651,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "table = [\n",
        "    ['Model Name', 'Accuracy','Real Accuracy','Predited Classes','Last Epoch','Best Epoch Train','Best Epoch Validate'],\n",
        "    [pretrained_fasttext_MLP_u.name,acc_pre_u,real_acc_pre_u,pred_pre_u,pretrained_fasttext_MLP_u.last_epoch,min(pretrained_fasttext_MLP_u.loss_during_training),min(pretrained_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP_o.name,acc_pre_o,real_acc_pre_o,pred_pre_o,pretrained_fasttext_MLP_o.last_epoch,min(pretrained_fasttext_MLP_o.loss_during_training),min(pretrained_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP.name,acc_pre,real_acc_pre,pred_pre,pretrained_fasttext_MLP.last_epoch,min(pretrained_fasttext_MLP.loss_during_training),min(pretrained_fasttext_MLP.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_u.name,acc_cust_u,real_acc_cust_u,pred_cust_u,custom_fasttext_MLP_u.last_epoch,min(custom_fasttext_MLP_u.loss_during_training),min(custom_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_o.name,acc_cust_o,real_acc_cust_o,pred_cust_o,custom_fasttext_MLP_o.last_epoch,min(custom_fasttext_MLP_o.loss_during_training),min(custom_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP.name,acc_cust,real_acc_cust,pred_cust,custom_fasttext_MLP.last_epoch,min(custom_fasttext_MLP.loss_during_training),min(custom_fasttext_MLP.valid_loss_during_training)]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒══════════════════════════════════════════════════╤════════════╤═════════════════╤══════════════════════════╤══════════════╤════════════════════╤═══════════════════════╕\n",
            "│ Model Name                                       │   Accuracy │   Real Accuracy │ Predited Classes         │   Last Epoch │   Best Epoch Train │   Best Epoch Validate │\n",
            "╞══════════════════════════════════════════════════╪════════════╪═════════════════╪══════════════════════════╪══════════════╪════════════════════╪═══════════════════════╡\n",
            "│ pretrained_fast_tsone_under_[100, 50]_1673102749 │   0.564892 │        0.799293 │ [21, 25, 29, 23, 31, 27] │           42 │            1.31812 │               1.47053 │\n",
            "├──────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_fast_tsone_over_[100, 50]_1673102749  │   0.589434 │        0.81926  │ [21, 29, 25, 23, 31, 27] │           32 │            1.34004 │               1.44661 │\n",
            "├──────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_tsone_fast_[100, 50]_1673097759       │   0.610233 │        0.826539 │ [21, 23, 25, 29]         │           59 │            1.40897 │               1.42234 │\n",
            "├──────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tsone_under_[100, 50]_1673102749     │   0.602745 │        0.834235 │ [21, 23, 25, 29, 31, 27] │           55 │            1.35573 │               1.43819 │\n",
            "├──────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tsone_over_[100, 50]_1673102749      │   0.597962 │        0.834443 │ [21, 25, 29, 23, 31, 27] │           26 │            1.39189 │               1.44131 │\n",
            "├──────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tsone_[100, 50]_1673102749           │   0.617928 │        0.834859 │ [21, 23, 25, 29]         │           45 │            1.41928 │               1.40921 │\n",
            "╘══════════════════════════════════════════════════╧════════════╧═════════════════╧══════════════════════════╧══════════════╧════════════════════╧═══════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 653,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_summary_results = pd.DataFrame(data = table[1:], columns = table[0])\n",
        "text_summary_results.to_csv('best_results_text_title&sumary_one_vector.csv',index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Separetly (two vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_fb = torch.cat((torch.tensor(train['pretrained_fasttext']),torch.tensor(train['pretrained_fasttext_title'])),1)\n",
        "train_custom = torch.cat((torch.tensor(train['custom_trained_fasttext']),torch.tensor(train['custom_trained_fasttext_title'])),1)\n",
        "train_labels = torch.tensor(train.main_theme)\n",
        "\n",
        "train_under_fb = torch.cat((torch.tensor(undersampled_train['pretrained_fasttext']),torch.tensor(undersampled_train['pretrained_fasttext_title'])),1)\n",
        "train_under_custom = torch.cat((torch.tensor(undersampled_train['custom_trained_fasttext']),torch.tensor(undersampled_train['custom_trained_fasttext_title'])),1)\n",
        "train_under_labels = torch.tensor(undersampled_train.main_theme)\n",
        "\n",
        "train_over_fb = torch.cat((torch.tensor(oversampled_train['pretrained_fasttext']),torch.tensor(oversampled_train['pretrained_fasttext_title'])),1)\n",
        "train_over_custom = torch.cat((torch.tensor(oversampled_train['custom_trained_fasttext']),torch.tensor(oversampled_train['custom_trained_fasttext_title'])),1)\n",
        "train_over_labels = torch.tensor(oversampled_train.main_theme)\n",
        "\n",
        "validate_fb = torch.cat((torch.tensor(validate['pretrained_fasttext']),torch.tensor(validate['pretrained_fasttext_title'])),1)\n",
        "validate_custom = torch.cat((torch.tensor(validate['custom_trained_fasttext']),torch.tensor(validate['custom_trained_fasttext_title'])),1)\n",
        "validate_labels = torch.tensor(validate.main_theme)\n",
        "\n",
        "test_fb = torch.cat((torch.tensor(test['pretrained_fasttext']),torch.tensor(test['pretrained_fasttext_title'])),1)\n",
        "test_custom = torch.cat((torch.tensor(test['custom_trained_fasttext']),torch.tensor(test['custom_trained_fasttext_title'])),1)\n",
        "test_labels = torch.tensor(test.main_theme)\n",
        "test_rest_labels = test.rest_themes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_fasttext_MLP_u = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_tstwo_under_')\n",
        "pretrained_fasttext_MLP_o = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_fast_tstwo_over_')\n",
        "pretrained_fasttext_MLP = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'pretrained_tstwo_fast_')\n",
        "custom_fasttext_MLP_u = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tstwo_under_')\n",
        "custom_fasttext_MLP_o = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tstwo_over_' )\n",
        "custom_fasttext_MLP = MLP(dimx = 600, hidden_dim = [200,100], nlabels = 6, dropout_prob = 0.2, epochs = 100, name = 'custom_fast_tstwo_' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_fasttext_MLP_u.trainloop(train_under_fb,train_under_labels,validate_fb,validate_labels)\n",
        "pretrained_fasttext_MLP_o.trainloop(train_over_fb,train_over_labels,validate_fb,validate_labels)\n",
        "pretrained_fasttext_MLP.trainloop(train_fb,train_labels,validate_fb,validate_labels)\n",
        "custom_fasttext_MLP_u.trainloop(train_under_custom, train_under_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP_o.trainloop(train_over_custom, train_over_labels, validate_custom, validate_labels)\n",
        "custom_fasttext_MLP.trainloop(train_custom, train_labels, validate_custom, validate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_pre_u, real_acc_pre_u, pred_pre_u = pretrained_fasttext_MLP_u.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre_o, real_acc_pre_o, pred_pre_o = pretrained_fasttext_MLP_o.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_pre, real_acc_pre, pred_pre = pretrained_fasttext_MLP.evaluate(test_fb,test_labels,test_rest_labels)\n",
        "acc_cust_u, real_acc_cust_u, pred_cust_u = custom_fasttext_MLP_u.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust_o, real_acc_cust_o, pred_cust_o = custom_fasttext_MLP_o.evaluate(test_custom,test_labels, test_rest_labels)\n",
        "acc_cust, real_acc_cust, pred_cust = custom_fasttext_MLP.evaluate(test_custom,test_labels, test_rest_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table = [\n",
        "    ['Model Name', 'Accuracy','Real Accuracy','Predited Classes','Last Epoch','Best Epoch Train','Best Epoch Validate'],\n",
        "    [pretrained_fasttext_MLP_u.name,acc_pre_u,real_acc_pre_u,pred_pre_u,pretrained_fasttext_MLP_u.last_epoch,min(pretrained_fasttext_MLP_u.loss_during_training),min(pretrained_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP_o.name,acc_pre_o,real_acc_pre_o,pred_pre_o,pretrained_fasttext_MLP_o.last_epoch,min(pretrained_fasttext_MLP_o.loss_during_training),min(pretrained_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [pretrained_fasttext_MLP.name,acc_pre,real_acc_pre,pred_pre,pretrained_fasttext_MLP.last_epoch,min(pretrained_fasttext_MLP.loss_during_training),min(pretrained_fasttext_MLP.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_u.name,acc_cust_u,real_acc_cust_u,pred_cust_u,custom_fasttext_MLP_u.last_epoch,min(custom_fasttext_MLP_u.loss_during_training),min(custom_fasttext_MLP_u.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP_o.name,acc_cust_o,real_acc_cust_o,pred_cust_o,custom_fasttext_MLP_o.last_epoch,min(custom_fasttext_MLP_o.loss_during_training),min(custom_fasttext_MLP_o.valid_loss_during_training)],\n",
        "    [custom_fasttext_MLP.name,acc_cust,real_acc_cust,pred_cust,custom_fasttext_MLP.last_epoch,min(custom_fasttext_MLP.loss_during_training),min(custom_fasttext_MLP.valid_loss_during_training)]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒═══════════════════════════════════════════════════╤════════════╤═════════════════╤══════════════════════════╤══════════════╤════════════════════╤═══════════════════════╕\n",
            "│ Model Name                                        │   Accuracy │   Real Accuracy │ Predited Classes         │   Last Epoch │   Best Epoch Train │   Best Epoch Validate │\n",
            "╞═══════════════════════════════════════════════════╪════════════╪═════════════════╪══════════════════════════╪══════════════╪════════════════════╪═══════════════════════╡\n",
            "│ pretrained_fast_tstwo_under_[200, 100]_1673137505 │   0.547213 │        0.77683  │ [21, 25, 29, 31, 23, 27] │           46 │            1.40456 │               1.45324 │\n",
            "├───────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_fast_tstwo_over_[200, 100]_1673137505  │   0.595466 │        0.824459 │ [21, 29, 25, 23, 31, 27] │           43 │            1.39787 │               1.44032 │\n",
            "├───────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ pretrained_tstwo_fast_[200, 100]_1673137505       │   0.602121 │        0.822379 │ [21, 23, 25, 29]         │           35 │            1.35467 │               1.44303 │\n",
            "├───────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tstwo_under_[200, 100]_1673137505     │   0.578619 │        0.812396 │ [21, 25, 29, 23, 27, 31] │           67 │            1.33543 │               1.43932 │\n",
            "├───────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tstwo_over_[200, 100]_1673137505      │   0.568636 │        0.806572 │ [21, 25, 29, 23, 31, 27] │           87 │            1.31821 │               1.14674 │\n",
            "├───────────────────────────────────────────────────┼────────────┼─────────────────┼──────────────────────────┼──────────────┼────────────────────┼───────────────────────┤\n",
            "│ custom_fast_tstwo_[200, 100]_1673137505           │   0.604409 │        0.831115 │ [21, 23, 25, 29]         │           37 │            1.40012 │               1.46482 │\n",
            "╘═══════════════════════════════════════════════════╧════════════╧═════════════════╧══════════════════════════╧══════════════╧════════════════════╧═══════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_summary_results = pd.DataFrame(data = table[1:], columns = table[0])\n",
        "text_summary_results.to_csv('best_results_text_title&sumary_two_vector.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EdcCTalUVYvz",
        "Wr49hO65-d8c",
        "TGnWKzs-bd4h",
        "OvHLskhkAinE",
        "euI9oX_gEcR-"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
